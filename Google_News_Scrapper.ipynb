{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bf82b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "import dateparser\n",
    "from langdetect import detect\n",
    "from datetime import datetime,timedelta, date\n",
    "import pandas as pd\n",
    "\n",
    "# This function takes a URL as input and returns a news link from Google News. \n",
    "# It uses the requests and BeautifulSoup modules to extract the link.\n",
    "def find_news_link(URL11):\n",
    "    try:\n",
    "        page=requests.get(URL11,timeout=60)\n",
    "        soup11 = BeautifulSoup(page.content, 'html.parser')\n",
    "        result11 = soup11.find('div', {'class':'Pg70bf Uv67qb'})\n",
    "        links = result11.find_all('a', {'class':'eZt8xd'})\n",
    "        link_list = links[0]['href']\n",
    "        return \"https://www.google.com/\"+link_list\n",
    "        time.sleep(10)\n",
    "    except requests.exceptions.Timeout:\n",
    "        return None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "# This function takes a news URL as input and returns a list of dictionaries that contain news data such as source, link, title, and date. \n",
    "# It uses the requests and BeautifulSoup modules to extract the data. It also uses a loop to fetch data from multiple pages of Google News.\n",
    "def find_news_data(url):\n",
    "    URL = find_news_link(url)\n",
    "    page=requests.get(URL,timeout=60)\n",
    "    \n",
    "\n",
    "    final_list=[]\n",
    "    page_num = 0\n",
    "    while page_num < 1:\n",
    "        page_num += 1\n",
    "        print(f\"{page_num} page:\")\n",
    "        soup11 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        \n",
    "        result = soup11.find_all('div', {'class':'Gx5Zad fP1Qef xpd EtOod pkphOe'})#Gx5Zad xpd EtOod pkphOe #Gx5Zad fP1Qef xpd EtOod pkphOe\n",
    "\n",
    "        for i in result:\n",
    "            des={}\n",
    "            source = i.find('div', {'class':'BNeawe UPmit AP7Wnd lRVwie'}).text\n",
    "            link = i.find('a')['href'].replace('/url?q=','')\n",
    "            title = i.find('h3').text\n",
    "            date=i.find('span', {'class':'r0bn4c rQMQod'}).text\n",
    "\n",
    "            final_link=link.split('&')[0]\n",
    "\n",
    "            des['source']=source\n",
    "            des['link']=final_link\n",
    "            des['title']=title\n",
    "            des['date']=date\n",
    "            final_list.append(des)\n",
    "        \n",
    "        try:\n",
    "            next_link = soup11.find('a', {'aria-label':'Next page'})['href']\n",
    "\n",
    "\n",
    "            URL1='https://www.google.com/'+next_link\n",
    "            page=requests.get(URL1,timeout=60)\n",
    "            print(URL1)\n",
    "        except:\n",
    "            break        \n",
    "    return final_list,soup11\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "# This function takes a news link as input and returns the headline of the news. \n",
    "# It uses the newspaper module to extract the headline.\n",
    "def headlines(link):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "    config = Config()\n",
    "    config.browser_user_agent = user_agent\n",
    "    link.strip()\n",
    "    page = Article(str(link), config=config)\n",
    "    try:\n",
    "        page.download()\n",
    "        page.parse()\n",
    "        return page.title\n",
    "    except:\n",
    "        return 'Untitled Page'\n",
    "\n",
    "\n",
    "# This function takes a URL as input and returns the title of the webpage. (used if the number of words in headline>10) \n",
    "# It uses the requests and BeautifulSoup modules to extract the title.\n",
    "# def get_page_title(url):\n",
    "#     try:\n",
    "#         r = requests.get(url,timeout=60)\n",
    "#         soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "#         if soup.title and soup.title.text:\n",
    "#             return soup.title.text\n",
    "#         else:\n",
    "#             return \"Error\"\n",
    "#     except requests.exceptions.Timeout:\n",
    "#         return \"Error\"\n",
    "#     except requests.exceptions.RequestException:\n",
    "#         return None\n",
    "\n",
    "def get_page_title(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout=60)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        if soup.find(\"h1\"):\n",
    "            return soup.find(\"h1\").text\n",
    "        else:\n",
    "            return \"Error\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "# The handle method first obtains the current date and then calculates the date of yesterday. \n",
    "# It uses the datetime module to achieve this.\n",
    "#now = date.today()\n",
    "\n",
    "now = datetime.now()\n",
    "now = now.date()\n",
    "yesterday = now - timedelta(days=1)\n",
    "yesterday = yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "y = []\n",
    "# It constructs a Google search URL by concatenating the keyword and industry category, \n",
    "# and then it passes this URL to the find_news_data function to scrape the news data from the Google search results page.\n",
    "URL = 'https://www.google.com/search?q='+'real estate gurgaon news'\n",
    "print(URL)\n",
    "\n",
    "\n",
    "data,soup11 = find_news_data(URL)\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "print(\"*\"*50)                  \n",
    "#print(data)\n",
    "\n",
    "\n",
    "# The code then processes the scraped news data by extracting the date of each article and filtering out any articles that \n",
    "# are not from the previous day. \n",
    "DATE = []\n",
    "for i in data:\n",
    "    if i['date']:\n",
    "        date = dateparser.parse(i['date'])\n",
    "    if date:\n",
    "        date = date.strftime(\"%Y-%m-%d\")\n",
    "        DATE.append(date)\n",
    "\n",
    "filtered_data = []\n",
    "for data1, modified_date in zip(data, DATE):\n",
    "    if modified_date:\n",
    "        data1['Modified Dates'] = modified_date\n",
    "        filtered_data.append(data1)\n",
    "\n",
    "\n",
    "\n",
    "    filtered_data_final = []\n",
    "    for data2 in filtered_data:\n",
    "        if data2['Modified Dates']:\n",
    "            modified_date = dateparser.parse(data2['Modified Dates'], date_formats=['%Y-%m-%d'])\n",
    "            modified_date = modified_date.strftime('%Y-%m-%d')\n",
    "            if modified_date == yesterday:\n",
    "                filtered_data_final.append(data2)\n",
    "\n",
    "\n",
    "\n",
    "# It filters out any articles that are not in English.\n",
    "list1 = []\n",
    "for item in filtered_data_final:\n",
    "    title = item['title']\n",
    "    if detect(title) == 'en':  \n",
    "        list1.append(item)     \n",
    "        \n",
    "\n",
    "# This fetches the headline if number of words>10.\n",
    "for item in list1:\n",
    "    if len(item['title'].split()) > 10:\n",
    "        item['title'] = get_page_title(item['link'])\n",
    "    else:\n",
    "        item['title'] = headlines(item['link']) \n",
    "    \n",
    "\n",
    "# Removes news of the given sources\n",
    "list1 = [x for x in list1 if isinstance(x, dict) and x.get('source') is not None and ('Analytics Insight' not in x['source']) and ('Informist' not in x['source']) and\n",
    "         ('Home' not in x['source']) and ('Medium' not in x['source']) and ('Macro Hive' not in x['source']) and\n",
    "         ('SourceSecurity.com' not in x['source']) and ('Airbus' not in x['source']) and \n",
    "         ('Aviation International News' not in x['source']) and ('Fitch Ratings' not in x['source']) and \n",
    "         ('www' not in x['source']) and ('Stock Market | FinancialContent Business Page' not in x['source']) \n",
    "         and ('502' not in x['source']) and ('POPSUGAR Australia' not in x['source']) and \n",
    "         ('Arab News' not in x['source']) and ('The Financial Express' not in x['source']) and \n",
    "         ('Seeking Alpha' not in x['source']) and ('Dalal Street Investment Journal' not in x['source']) and \n",
    "         ('Detroit News' not in x['source']) and ('Beaver County Times' not in x['source']) and \n",
    "         ('The New York Times' not in x['source']) and ('52 week high stocks: Stock market update: Stocks that hit 52-week ...' not in x['source']) \n",
    "         and ('European Commission' not in x['source']) and ('Financial Times' not in x['source'])\n",
    "         and ('chescotimes.com |' not in x['source']) and ('Times Higher Education' not in x['source'])\n",
    "         and ('UBC In The News' not in x['source']) and ('The Fayetteville Observer' not in x['source'])\n",
    "         and ('Top Gear' not in x['source']) and ('Construction World' not in x['source'])\n",
    "         and ('Illinois.gov' not in x['source']) and ('Dalhousie University' not in x['source'])\n",
    "         and ('Redfin' not in x['source']) and ('Yahoo Finance' not in x['source'])]       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Removes news of the given title\n",
    "list1 = [x for x in list1 if isinstance(x, dict) and x.get('title') is not None and ('Error' not in x['title']) and ('Captcha' not in x['title']) and\n",
    "         ('Are you a robot?' not in x['title']) and ('Untitled Page' not in x['title']) and \n",
    "         ('Subscribe' not in x['title']) and ('You are being redirected...' not in x['title']) and \n",
    "         ('Not Acceptable!' not in x['title']) and ('403 Forbidden' not in x['title']) and \n",
    "         ('ERROR: The request could not be satisfied' not in x['title']) and ('Just a moment...' not in x['title']) and \n",
    "         ('403 - Forbidden: Access is denied.' not in x['title']) and ('Not Found' not in x['title']) and \n",
    "         ('Page Not Found' not in x['title']) and ('StackPath' not in x['title']) and ('Access denied' not in x['title'])\n",
    "         and ('Yahoo' not in x['title']) and ('Stock Market Insights' not in x['title']) and \n",
    "         ('Attention Required!' not in x['title']) and ('Access Denied' not in x['title'])\n",
    "         and ('403 forbidden' not in x['title']) and ('Too Many Requests' not in x['title'])\n",
    "         and ('403 - Forbidden' not in x['title']) and ('NCSC' not in x['title'])\n",
    "         and ('BC Gov News' not in x['title']) and ('The Verge' not in x['title']) and ('Trackinsight' not in x['title'])\n",
    "         and ('Morning Headlines' not in x['title']) and ('Forbidden' not in x['title'])\n",
    "         and ('forbidden' not in x['title']) and ('Detroit Free Press' not in x['title'])\n",
    "         and ('reuters.com' not in x['title']) and ('403 unauthorized' not in x['title'])\n",
    "         and ('403 not available now' not in x['title']) and ('Not Acceptable' not in x['title']) \n",
    "         and ('Your access to this site has been limited by the site owner' not in x['title'])\n",
    "         and ('404 - File or directory not found.' not in x['title'])]\n",
    "\n",
    "\n",
    "# Changing the this relevant source name\n",
    "for item in list1:\n",
    "    if 'Fortune India: Business News, Strategy, Finance and Corporate ...' in item['source']:\n",
    "        item['source'] = 'Fortune India'\n",
    "#print(\"-\"*50)\n",
    "#print(list1)  \n",
    "# Add the keyword to each item in the list\n",
    "\n",
    "\n",
    "# Extend the all_data list with the current list1\n",
    "y.extend(list1)\n",
    "df = pd.DataFrame(y)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
